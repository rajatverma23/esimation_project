{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7932592,"sourceType":"datasetVersion","datasetId":4660066},{"sourceId":7932642,"sourceType":"datasetVersion","datasetId":4662881}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# importing libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport numpy as np\nimport os\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T20:00:42.233223Z","iopub.execute_input":"2024-05-20T20:00:42.233674Z","iopub.status.idle":"2024-05-20T20:00:44.635722Z","shell.execute_reply.started":"2024-05-20T20:00:42.233638Z","shell.execute_reply":"2024-05-20T20:00:44.634721Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# importing elpileptic and healthy data\ndef load_csv_files(directory):\n    all_df = []    \n    for filename in os.listdir(directory):\n        if filename.endswith(\".csv\"):\n            file_path = os.path.join(directory, filename)\n            df = pd.read_csv(file_path)\n            all_df.append(df)\n    \n    combined_df = pd.concat(all_df, ignore_index=True)   \n    return combined_df\n\nepi_directory = '/kaggle/input/epileptic'\nhealthy_directory = '/kaggle/input/healthy'\n\n# loading epileptical data \nepi_data = load_csv_files(epi_directory)\n# loading healthy data\nheal_data = load_csv_files(healthy_directory)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:00:44.637414Z","iopub.execute_input":"2024-05-20T20:00:44.638455Z","iopub.status.idle":"2024-05-20T20:01:17.766575Z","shell.execute_reply.started":"2024-05-20T20:00:44.638418Z","shell.execute_reply":"2024-05-20T20:01:17.765285Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# converting the dataframe into 2-D numpy-array\nepi_data = np.asarray(epi_data.T)\nheal_data = np.asarray(heal_data.T)\n# adjusting sizes\nepi_resample_data = np.zeros_like(heal_data)\nfor i in np.arange(0, 19):\n    epi_resample_data[i] = epi_data[i][ : 1207161]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epi_resample_data.shape, heal_data.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:43:12.894709Z","iopub.execute_input":"2024-05-19T19:43:12.895282Z","iopub.status.idle":"2024-05-19T19:43:12.903699Z","shell.execute_reply.started":"2024-05-19T19:43:12.895249Z","shell.execute_reply":"2024-05-19T19:43:12.902435Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"((19, 1207161), (19, 1207161))"},"metadata":{}}]},{"cell_type":"markdown","source":"### Second Feautre : Approximate Entropy","metadata":{}},{"cell_type":"code","source":"def approximate_entropy(time_series, m, r):\n    \"\"\"  \n    Parameters:\n    - time_series: The time series data (a list or numpy array).\n    - m: Length of compared run of data (usually a small integer).\n    - r: Filtering level (a positive number, often 0.2 * std of the time series).\n    Returns:\n    - ApEn value.\n    \"\"\"  \n    def _phi(m):\n        N = len(time_series)\n        x = [time_series[i : i + m] for i in range(N - m + 1)]\n        C = [np.sum(np.max(np.abs(x - x[j]), axis=1) <= r) / (N - m + 1.0) for j in range(len(x))]\n        return np.sum(np.log(C)) / (N - m + 1.0) \n    return _phi(m) - _phi(m + 1) \n\ndef break_time_series(time_series, length=250):\n    # Calculate the length of padding needed\n    padding_needed = length - (len(time_series) % length)\n    \n    # If padding is needed, calculate the mean of the time series\n    if padding_needed > 0:\n        mean_value = np.mean(time_series)\n        padding = [mean_value] * padding_needed\n        time_series = np.concatenate([time_series, padding])\n    \n    # Break the time series into equal intervals of 50\n    num_intervals = len(time_series) // length\n    broken_series = np.array_split(time_series, num_intervals)  \n    #features::\n    #interval_means     =    [np.mean(interval) for interval in broken_series]\n    #svd_features       =    [np.mean(interval) for interval in broken_series]\n    entropy_feature    =    [approximate_entropy(interval,1000,0.2 * np.std(interval)) for interval in broken_series]  \n    return entropy_feature   \n\n#Feature extractor for all channels:\ndef feature_extractor_concat(data, length):\n    feature=[]\n    for i in range(0,19):\n        m=break_time_series(pd.DataFrame(data).T.iloc[i], length)\n        feature.append(m)\n    feature=pd.DataFrame(feature)\n    return feature","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:43:12.906268Z","iopub.execute_input":"2024-05-19T19:43:12.906645Z","iopub.status.idle":"2024-05-19T19:43:12.921324Z","shell.execute_reply.started":"2024-05-19T19:43:12.906616Z","shell.execute_reply":"2024-05-19T19:43:12.920062Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# calculating approx_entropy for a all channels\nlength = heal_data.shape[1]\nepi_appEntropy_feautre = feature_extractor_concat(epi_resample_data, length)\nhealt_appEntropy_feautre = feature_extractor_concat(heal_data, length)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:43:53.979367Z","iopub.execute_input":"2024-05-19T19:43:53.979893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epi_appEntropy_feautres = []\n# healt_appEntropy_feautres = []\n# length = heal_data.shape[1]\n\n# for i in range(0, 2):\n#     epi_appEntropy_feautre = feature_extractor_concat(epi_resample_data[i], length)\n#     healt_appEntropy_feautre = feature_extractor_concat(heal_data[i], length)\n#     epi_appEntropy_feautres.append(epi_appEntropy_feautre)\n#     healt_appEntropy_feautres.append(healt_appEntropy_feautre)\n    \n# epi_appEntropy_feautres = np.asarray(epi_appEntropy_feautres)\n# healt_appEntropy_feautres = np.asarray(healt_appEntropy_feautres)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}